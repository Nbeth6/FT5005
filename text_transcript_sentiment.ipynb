{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6624c17e",
   "metadata": {},
   "source": [
    "# Earning Call Transcripts Sentiment\n",
    "\n",
    "For: Tan Cheen Hao!\n",
    "\n",
    "The transcripts are already given to us by quarter by company so aggregation is not needed.\n",
    "\n",
    "In the very basic form we basically want the output to be a csv file in the format below. (ideally order by quarter_year then by ticker but doesn't matter). `transcript_sentiment` should be values between 0 to 1 where the value vaguely represents the probability of a positive sentiment. Or -1 to 1 where -1 is neg and 1 is pos. This depends on you but _make it clear with a markdown at the end._\n",
    "\n",
    "| ticker | quarter_year | transcript_sentiment |\n",
    "| ------ | ------------ | -------------------- |\n",
    "| BAC    | Q1 2001      | 0.2                  |\n",
    "| JPM    | Q1 2001      | 0.67                 |\n",
    "| WFC    | Q1 2001      | 0.97                 |\n",
    "\n",
    "Now, you could also explore the use of LLMs and prompt engineering to extract specific information from the text first. For example, you could look into using LLMs to extract company specific info vs market info or ask the LLM to find how \"confident\" the announcer is before extracting the sentiment.\n",
    "\n",
    "For earning calls, instead of finding whether its positive or negative, you could also find the degree of complexity, or even degree of confidence. Also, look into **aspect based sentiment analysis**, it could be useful. Ideally, you should have 2 output files; 1 for revenue and 1 for CAR.\n",
    "\n",
    "Be creative!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0982434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a603fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the JSON files\n",
    "json_folder_path ='ECC Transcript\\Banks' \n",
    "\n",
    "# List to store transcripts\n",
    "transcripts = []\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(json_folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(json_folder_path, filename)\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Combine all component texts into one document\n",
    "        components = data.get(\"components\", [])\n",
    "        full_text = \" \".join(component[\"text\"] for component in components if \"text\" in component)\n",
    "        \n",
    "        transcripts.append({\n",
    "            \"filename\": filename,\n",
    "            \"transcript\": full_text\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(transcripts)\n",
    "\n",
    "# Ensure you have the necessary NLTK data files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['processed_text'] = df['transcript'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e95f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _temp_fn(top_words: list[tuple]):\n",
    "    def _replace(doc):\n",
    "        for word, _ in top_words:\n",
    "            doc = doc.replace(word, \"\")\n",
    "        return doc\n",
    "    return _replace\n",
    "    \n",
    "def remove_popular_words(df, top_words: list[tuple]):\n",
    "    df = df.copy()\n",
    "    df[\"no_pop_words\"] = df[\"processed_text\"].apply(_temp_fn(top_words))\n",
    "    return df\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "# Compute TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000) \n",
    "tfidf_matrix = vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Add filenames back for reference\n",
    "tfidf_df['filename'] = df['filename']\n",
    "\n",
    "# Compute number of tokens per document (based on processed text)\n",
    "df['num_tokens'] = df['processed_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "\n",
    "# Remove popular words from the DataFrame\n",
    "df = remove_popular_words(df, [(\"--\", 1)])\n",
    "\n",
    "\n",
    "# Recompute token count after removing popular words\n",
    "df['num_tokens_no_pop'] = df['no_pop_words'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Flatten all tokens from processed text again after removing popular words\n",
    "all_tokens = \" \".join(df['no_pop_words']).split()\n",
    "\n",
    "vocab = set(all_tokens)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Basic Statistics\n",
    "total_documents = len(df)\n",
    "total_tokens = df['num_tokens_no_pop'].sum()\n",
    "avg_doc_length = df['num_tokens_no_pop'].mean()\n",
    "\n",
    "# Get top 50 most common words from processed text (not TF-IDF)\n",
    "top_words = Counter(all_tokens).most_common(50)\n",
    "\n",
    "# Get top 50 words based on average TF-IDF score\n",
    "avg_tfidf = tfidf_df.drop(columns=['filename']).mean().sort_values(ascending=False)\n",
    "top_tfidf_words = avg_tfidf.head(50)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Total documents      : {total_documents}\")\n",
    "print(f\"Total tokens         : {total_tokens}\")\n",
    "print(f\"Average doc length   : {avg_doc_length:.2f} tokens\")\n",
    "print(f\"Vocabulary size      : {vocab_size}\")\n",
    "\n",
    "print(\"\\nTop 50 most common words (raw frequency):\")\n",
    "for word, freq in top_words:\n",
    "    print(f\"{word:<15} {freq}\")\n",
    "\n",
    "print(\"\\nTop 50 words by TF-IDF importance:\")\n",
    "print(top_tfidf_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa2405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of word: average_tfidf_score\n",
    "tfidf_scores = tfidf_df.drop(columns=['filename']).mean().to_dict()\n",
    "\n",
    "# Generate raw WordCloud\n",
    "wordcloud = WordCloud(max_words=80, max_font_size=100, width=800, height=400, background_color='white')\n",
    "wordcloud.generate_from_frequencies(dict(top_words))\n",
    "\n",
    "# Generate TF-IDF-based WordCloud\n",
    "wordcloud1 = WordCloud(max_words=80, max_font_size=100, width=800, height=400, background_color='white')\n",
    "wordcloud1.generate_from_frequencies(tfidf_scores)\n",
    "\n",
    "# Display the WordCloud\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Count\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Display the WordCloud\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud1, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Average Word TF-IDF\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48e15a",
   "metadata": {},
   "source": [
    "Feed transcipt thu FinBert NLP model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d9d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d18661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "scale = StandardScaler()\n",
    "\n",
    "# Generate synthetic text for each document based on TF-IDF scores\n",
    "synthetic_texts = []\n",
    "for idx, row in tfidf_df.iterrows():\n",
    "    tfidf_scores = row.drop('filename').to_dict()\n",
    "    synthetic_text = \" \".join([\n",
    "        f\"{word} \" * int(score * 100)\n",
    "        for word, score in tfidf_scores.items()\n",
    "        if score > 0\n",
    "    ])\n",
    "    synthetic_texts.append(synthetic_text)\n",
    "\n",
    "# Compute raw complexity scores\n",
    "raw_complexities = []\n",
    "for synthetic_text in synthetic_texts:\n",
    "    complexity = textstat.flesch_reading_ease(synthetic_text)\n",
    "    raw_complexities.append([complexity])  # 2D for scaler\n",
    "\n",
    "# Scale complexity scores\n",
    "scaled_complexities = scale.fit_transform(raw_complexities)\n",
    "\n",
    "# Sentiment analysis and result assembly\n",
    "sentiment_results = []\n",
    "for idx, synthetic_text in enumerate(synthetic_texts):\n",
    "    # Tokenize the synthetic text\n",
    "    inputs = tokenizer(synthetic_text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = softmax(logits, dim=1)\n",
    "\n",
    "        sentiment_score = (-1 * probs[0][0].item()) + (1 * probs[0][2].item())\n",
    "        confidence = torch.max(probs).item()\n",
    "\n",
    "    complexity = scaled_complexities[idx][0]\n",
    "\n",
    "    # Parse filename to get company, quarter, and year\n",
    "    filename = tfidf_df.loc[idx, 'filename'].replace(\".json\", \"\")\n",
    "    quarter_match = re.search(r'(Q[1-4]) (\\d{4})', filename)\n",
    "\n",
    "    if quarter_match:\n",
    "        quarter = quarter_match.group(1)\n",
    "        year = quarter_match.group(2)\n",
    "    else:\n",
    "        quarter = \"Unknown\"\n",
    "        year = \"Unknown\"\n",
    "\n",
    "    company_match = re.search(r'companyid_(\\d+)', filename)\n",
    "    company_id = company_match.group(1) if company_match else \"Unknown\"\n",
    "\n",
    "    sentiment_results.append({\n",
    "        \"company\": company_id,\n",
    "        \"quarter\": quarter,\n",
    "        \"year\": year,\n",
    "        \"sentiment_score\": sentiment_score,\n",
    "        \"confidence\": confidence,\n",
    "        \"complexity\": complexity\n",
    "    })\n",
    "\n",
    "    print(f\"{company_id} | {quarter} | {year} | Sentiment: {sentiment_score:.3f} | Confidence: {confidence:.3f} | Complexity: {complexity:.2f}\")\n",
    "\n",
    "\n",
    "sentiment_df = pd.DataFrame(sentiment_results)\n",
    "sentiment_df.to_csv(\"sentiment_results.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192d932f",
   "metadata": {},
   "source": [
    "PAOPAO your code at the bottom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639176a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce395ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8501b10f87db4c9bad9902973607cec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Directory containing the JSON files\n",
    "json_folder_path ='data/text/earning_call_transcripts' \n",
    "\n",
    "# List to store transcripts\n",
    "transcripts = []\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in tqdm(os.listdir(json_folder_path)):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(json_folder_path, filename)\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Combine all component texts into one document\n",
    "        components = data.get(\"components\", [])\n",
    "        full_text = \"[BREAK]\".join(component[\"text\"] for component in components if \"text\" in component)\n",
    "        most_important_date = data.get(\"mostimportantdate\", np.nan)\n",
    "        company_id = data.get(\"companyid\", np.nan)\n",
    "\n",
    "        transcripts.append({\n",
    "            \"company_id\": company_id,\n",
    "            \"date\": most_important_date,\n",
    "            \"transcript\": full_text,\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "transcripts_data = pd.DataFrame(transcripts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c783db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first transcript date: 2006-10-19\n",
      "last transcript date: 2025-02-05\n",
      "number of transcripts: 3868\n"
     ]
    }
   ],
   "source": [
    "print(\"first transcript date:\", transcripts_data[\"date\"].min())\n",
    "print(\"last transcript date:\", transcripts_data[\"date\"].max())\n",
    "print(\"number of transcripts:\", len(transcripts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c88362a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_transcript(text):\n",
    "    \"\"\"Write the text preprocessing function here. This should work through the `df.apply()` function\"\"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd68a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_transcript(transcripts_data: pd.DataFrame):\n",
    "    \"\"\"This function should take in the news data and output the final csv file dataframe\"\"\"\n",
    "    output_data = transcripts_data.copy()\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ddadae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the final output\n",
    "\n",
    "# output_data = sentiment_analysis_transcript(news_data)\n",
    "# output_data.to_csv(\"output_transcript_sentiment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15cc44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
