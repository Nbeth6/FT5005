{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6624c17e",
   "metadata": {},
   "source": [
    "# Earning Call Transcripts Sentiment\n",
    "\n",
    "For: Tan Cheen Hao!\n",
    "\n",
    "The transcripts are already given to us by quarter by company so aggregation is not needed.\n",
    "\n",
    "In the very basic form we basically want the output to be a csv file in the format below. (ideally order by quarter_year then by ticker but doesn't matter). `transcript_sentiment` should be values between 0 to 1 where the value vaguely represents the probability of a positive sentiment. Or -1 to 1 where -1 is neg and 1 is pos. This depends on you but *make it clear with a markdown at the end.*\n",
    "\n",
    "\n",
    "| ticker | quarter_year  | transcript_sentiment |\n",
    "|--------|---------------|----------------------|\n",
    "| BAC    | Q1 2001       | 0.2                  |\n",
    "| JPM    | Q1 2001       | 0.67                 |\n",
    "| WFC    | Q1 2001       | 0.97                 |\n",
    "\n",
    "Now, you could also explore the use of LLMs and prompt engineering to extract specific information from the text first. For example, you could look into using LLMs to extract company specific info vs market info or ask the LLM to find how \"confident\" the announcer is before extracting the sentiment.\n",
    "\n",
    "For earning calls, instead of finding whether its positive or negative, you could also find the degree of complexity, or even degree of confidence. Also, look into **aspect based sentiment analysis**, it could be useful. Ideally, you should have 2 output files; 1 for revenue and 1 for CAR.\n",
    "\n",
    "Be creative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639176a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce395ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8501b10f87db4c9bad9902973607cec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Directory containing the JSON files\n",
    "json_folder_path ='data/text/earning_call_transcripts' \n",
    "\n",
    "# List to store transcripts\n",
    "transcripts = []\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in tqdm(os.listdir(json_folder_path)):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(json_folder_path, filename)\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Combine all component texts into one document\n",
    "        components = data.get(\"components\", [])\n",
    "        full_text = \"[BREAK]\".join(component[\"text\"] for component in components if \"text\" in component)\n",
    "        most_important_date = data.get(\"mostimportantdate\", np.nan)\n",
    "        company_id = data.get(\"companyid\", np.nan)\n",
    "\n",
    "        transcripts.append({\n",
    "            \"company_id\": company_id,\n",
    "            \"date\": most_important_date,\n",
    "            \"transcript\": full_text,\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "transcripts_data = pd.DataFrame(transcripts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c783db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first transcript date: 2006-10-19\n",
      "last transcript date: 2025-02-05\n",
      "number of transcripts: 3868\n"
     ]
    }
   ],
   "source": [
    "print(\"first transcript date:\", transcripts_data[\"date\"].min())\n",
    "print(\"last transcript date:\", transcripts_data[\"date\"].max())\n",
    "print(\"number of transcripts:\", len(transcripts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c88362a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_transcript(text):\n",
    "    \"\"\"Write the text preprocessing function here. This should work through the `df.apply()` function\"\"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd68a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_transcript(transcripts_data: pd.DataFrame):\n",
    "    \"\"\"This function should take in the news data and output the final csv file dataframe\"\"\"\n",
    "    output_data = transcripts_data.copy()\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ddadae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the final output\n",
    "\n",
    "# output_data = sentiment_analysis_transcript(news_data)\n",
    "# output_data.to_csv(\"output_transcript_sentiment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15cc44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
